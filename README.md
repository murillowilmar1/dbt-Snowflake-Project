# dbt_snowflake_project â€” Snowflake TPCH (Enterprise demo)

Proyecto dbt estilo â€œempresaâ€ sobre Snowflake usando datos de ejemplo:
`SNOWFLAKE_SAMPLE_DATA.TPCH_SF1`

Este repo implementa un pipeline completo:
- limpieza robusta (staging)
- transformaciÃ³n y enriquecimiento (intermediate)
- dimensiones con historial (snapshots / SCD Type 2)
- modelo dimensional tipo Kimball (marts: dims + facts)
- facts incrementales con estrategia MERGE
- data testing y documentaciÃ³n (dbt docs)

---

## 1) Arquitectura por capas (quÃ© significa cada una)

### A) `sources` (entrada)
Definimos las tablas raw (TPCH) como fuentes dbt. Esto permite:
- documentar origen
- tener tests bÃ¡sicos sobre fuentes
- usar `source()` con lineage claro

**Origen real**:
- `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER`
- `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS`
- `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.LINEITEM`
- `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION`
- `SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.REGION`

---

### B) `staging` (limpieza y estandarizaciÃ³n)
Objetivo: dejar â€œrawâ€ usable y consistente.

AquÃ­ hacemos:
- trim de espacios
- upper/lower donde aplica
- parseo/normalizaciÃ³n de fechas
- tipado fuerte (NUMBER/DATE/etc.)
- deduplicaciÃ³n (cuando aplica)
- columnas renombradas a nombres estables (`customer_id`, `order_date`, etc.)

MaterializaciÃ³n tÃ­pica: **views**.

---

### C) `intermediate` (enriquecer y preparar para modelo dimensional)
Objetivo: modelos â€œlistos para negocioâ€ pero aÃºn no finales.

AquÃ­ hacemos:
- joins entre entidades (ej: customer + nation + region)
- mÃ©tricas base (ej: net_amount en lineitems)
- agregaciones (ej: fact_orders a nivel orden)
- lÃ³gica reusable antes de construir marts

MaterializaciÃ³n tÃ­pica: **views**.

---

### D) `snapshots` (SCD Type 2)
Objetivo: historial de cambios en dimensiones.

Snapshots crean:
- `dbt_valid_from`
- `dbt_valid_to`
- (y la tabla snapshot se va actualizando con cambios)

En este proyecto, snapshots se usan para:
- Customer (dim_customer con historial)
- Geography (dim_geography con historial)

> Nota prÃ¡ctica: TPCH es estÃ¡tico. Para un demo estable, usamos la versiÃ³n vigente (`dbt_valid_to is null`) en facts.

---

### E) `marts` (modelo dimensional consumible)
Objetivo: capa final para BI/analytics.

Incluye:
- `dims/`
  - `dim_customer` (basada en snapshot)
  - `dim_geography` (basada en snapshot)
  - `dim_date` (calendario)
- `facts/`
  - `fact_sales` (lÃ­nea; incremental merge)
  - `fact_orders` (orden; incremental merge)

MaterializaciÃ³n:
- dims: views (o tables si quieres performance)
- facts: **incremental** con `merge`

---

## 2) Estructura de carpetas del proyecto

```text
dbt_snowflake_project/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ packages.yml
â”œâ”€â”€ selectors.yml                # (opcional) selectors por capa
â”œâ”€â”€ README.md
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ cleaning.sql             # trim/upper/to_date_safe
â”‚   â””â”€â”€ surrogate_keys.sql       # sk() surrogate keys
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ sources.yml          # definiciones de source()
â”‚   â”‚   â”œâ”€â”€ staging.yml          # tests/docs staging
â”‚   â”‚   â””â”€â”€ tpch/
â”‚   â”‚       â”œâ”€â”€ stg_tpch_customers.sql
â”‚   â”‚       â”œâ”€â”€ stg_tpch_orders.sql
â”‚   â”‚       â”œâ”€â”€ stg_tpch_lineitems.sql
â”‚   â”‚       â”œâ”€â”€ stg_tpch_nations.sql
â”‚   â”‚       â””â”€â”€ stg_tpch_regions.sql
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”œâ”€â”€ intermediate.yml
â”‚   â”‚   â”œâ”€â”€ int_dim_customer.sql
â”‚   â”‚   â”œâ”€â”€ int_dim_geography.sql
â”‚   â”‚   â”œâ”€â”€ int_fact_sales.sql
â”‚   â”‚   â””â”€â”€ int_fact_orders.sql
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ marts.yml            # tests/docs marts
â”‚       â”œâ”€â”€ dims/
â”‚       â”‚   â”œâ”€â”€ dim_customer.sql
â”‚       â”‚   â”œâ”€â”€ dim_geography.sql
â”‚       â”‚   â””â”€â”€ dim_date.sql
â”‚       â””â”€â”€ facts/
â”‚           â”œâ”€â”€ fact_sales.sql
â”‚           â””â”€â”€ fact_orders.sql
â”œâ”€â”€ snapshots/
â”‚   â”œâ”€â”€ snap_dim_customer.sql
â”‚   â””â”€â”€ snap_dim_geography.sql
â””â”€â”€ tests/
    â”œâ”€â”€ test_dim_customer_only_one_current_version.sql
    â”œâ”€â”€ test_fact_sales_no_future_ship_date.sql
    â”œâ”€â”€ test_fact_orders_no_future_order_date.sql
    â””â”€â”€ test_fact_sales_net_amount_non_negative.sql


---

## 3) Requisitos (Snowflake + dbt Cloud)

### Snowflake

Necesitas:

- Acceso a `SNOWFLAKE_SAMPLE_DATA`
- Un Warehouse (ej: `DBT_WH`)
- Un Database destino (ej: `SNOWFLAKE_DBT_TEST`)
- Un Schema base (ej: `DBT_SCHEMA`)

### Schemas por capa (recomendado)

dbt Cloud, cuando usas `+schema:` por capa, crea automÃ¡ticamente:

- `DBT_SCHEMA_STAGING`
- `DBT_SCHEMA_INTERMEDIATE`
- `DBT_SCHEMA_MARTS`
- `DBT_SCHEMA_SNAPSHOTS`

Si no existen, debes crearlos manualmente:

```sql
create schema if not exists SNOWFLAKE_DBT_TEST.DBT_SCHEMA_STAGING;
create schema if not exists SNOWFLAKE_DBT_TEST.DBT_SCHEMA_INTERMEDIATE;
create schema if not exists SNOWFLAKE_DBT_TEST.DBT_SCHEMA_MARTS;
create schema if not exists SNOWFLAKE_DBT_TEST.DBT_SCHEMA_SNAPSHOTS;


---

# 4) Dependencias (Packages)

Este proyecto utiliza paquetes externos para extender las capacidades de dbt.

## InstalaciÃ³n de dependencias

```bash
dbt deps
```

## packages.yml

```yaml
packages:
  - package: dbt-labs/dbt_utils
    version: ">=1.0.0"

  - package: metaplane/dbt_expectations
    version: ">=0.10.0"
```

---

## Â¿QuÃ© aporta cada paquete?

### ðŸ”¹ dbt_utils

Utilizado para:

- `generate_surrogate_key()` â†’ crear llaves sustitutas
- `date_spine()` â†’ construir dim_date
- `accepted_range` â†’ validaciones numÃ©ricas

Ejemplo:

```sql
{{ dbt_utils.generate_surrogate_key(['customer_id', 'dbt_valid_from']) }}
```

---

### ðŸ”¹ dbt_expectations

Permite validaciones avanzadas estilo Great Expectations.

Ejemplo:

```yaml
- name: discount
  tests:
    - dbt_expectations.expect_column_values_to_be_between:
        min_value: 0
        max_value: 1
```

---

# 5) CÃ³mo ejecutar el pipeline

## Desarrollo por capas (modo recomendado)

Permite validar cada etapa antes de avanzar.

```bash
# 1. Staging
dbt run --select staging
dbt test --select staging

# 2. Intermediate
dbt run --select intermediate
dbt test --select intermediate

# 3. Snapshots (SCD2)
dbt snapshot

# 4. Marts
dbt run --select marts
dbt test --select marts
```

---

## Modo Enterprise (CI/CD)

Este es el comando recomendado para producciÃ³n:

```bash
dbt build
```

### Â¿QuÃ© hace `dbt build`?

- Ejecuta models
- Ejecuta snapshots
- Ejecuta tests
- Respeta dependencias automÃ¡ticamente

Es el equivalente a un pipeline completo.

---

# 6) Validaciones en Snowflake

## Ver schemas creados por capa

```sql
show schemas like 'DBT_SCHEMA_%' in database SNOWFLAKE_DBT_TEST;
```

Esperado:

- DBT_SCHEMA_STAGING
- DBT_SCHEMA_INTERMEDIATE
- DBT_SCHEMA_MARTS
- DBT_SCHEMA_SNAPSHOTS

---

## Validaciones bÃ¡sicas de conteo

```sql
select count(*) from SNOWFLAKE_DBT_TEST.DBT_SCHEMA_MARTS.FACT_SALES;
select count(*) from SNOWFLAKE_DBT_TEST.DBT_SCHEMA_MARTS.FACT_ORDERS;
select count(*) from SNOWFLAKE_DBT_TEST.DBT_SCHEMA_MARTS.DIM_CUSTOMER;
```

---

## Revenue por regiÃ³n

```sql
select
  g.region_name,
  sum(f.net_amount) as revenue
from SNOWFLAKE_DBT_TEST.DBT_SCHEMA_MARTS.FACT_SALES f
join SNOWFLAKE_DBT_TEST.DBT_SCHEMA_MARTS.DIM_GEOGRAPHY g
  on f.geo_scd_key = g.geo_scd_key
group by 1
order by 2 desc;
```

---

# 7) DocumentaciÃ³n (dbt Docs)

âš ï¸ En dbt Cloud NO se usa `dbt docs serve`.

## Generar documentaciÃ³n

```bash
dbt docs generate
```

## Publicar documentaciÃ³n correctamente (recomendado)

Crear un Job en:

**Orchestration â†’ Jobs**

Comandos:

```bash
dbt build
dbt docs generate
```

DespuÃ©s del Job exitoso:

ðŸ‘‰ Ir a **Documentation** en el menÃº izquierdo.

AhÃ­ se verÃ¡:

- Lineage completo
- Snapshots
- Tests
- Dependencias
- Exposures

---

# 8) Data Quality & Testing

## Tests genÃ©ricos (definidos en YAML)

### Not Null

```yaml
- name: order_id
  tests:
    - not_null
```

---

### Unique

```yaml
- name: sales_line_key
  tests:
    - unique
```

---

### Rango aceptado

```yaml
- name: net_amount
  tests:
    - dbt_utils.accepted_range:
        arguments:
          min_value: 0
          inclusive: true
```

---

### ValidaciÃ³n entre valores

```yaml
- name: discount
  tests:
    - dbt_expectations.expect_column_values_to_be_between:
        min_value: 0
        max_value: 1
```

---

## Tests singulares (SQL)

Ubicados en `/tests`

### Solo una versiÃ³n vigente por customer

```sql
select customer_id
from {{ ref('dim_customer') }}
where is_current
group by customer_id
having count(*) > 1
```

---

### No fechas futuras

```sql
select *
from {{ ref('fact_sales') }}
where ship_date > current_date()
```

---

# 9) Macros utilizadas

## clean_trim

```sql
{% macro clean_trim(col) %}
  nullif(trim({{ col }}), '')
{% endmacro %}
```

Elimina espacios y convierte vacÃ­o en NULL.

---

## clean_trim_upper

```sql
{% macro clean_trim_upper(col) %}
  upper(nullif(trim({{ col }}), ''))
{% endmacro %}
```

Normaliza texto a mayÃºsculas.

---

## sk (Surrogate Key)

```sql
{% macro sk(cols) %}
  {{ dbt_utils.generate_surrogate_key(cols) }}
{% endmacro %}
```

Genera hash determinÃ­stico estable.

---

# 10) PrÃ³ximos pasos profesionales

## A) Job de ProducciÃ³n programado

Ejemplo:

- Frecuencia: Diario
- Comandos:

```bash
dbt build
dbt docs generate
```

---

## B) Exposures (conectar dashboards al lineage)

Archivo: `models/marts/exposures.yml`

```yaml
version: 2

exposures:
  - name: executive_sales_dashboard
    type: dashboard
    maturity: high
    depends_on:
      - ref('fact_sales')
      - ref('dim_customer')
      - ref('dim_geography')
    owner:
      name: Data Team
      email: data@company.com
```

---

## C) CI/CD (Pull Requests)

Ejecutar solo modelos modificados:

```bash
dbt build --select state:modified+
```

Esto permite Slim CI.

---

## D) Data Contracts

Permiten forzar esquema y tipos esperados en producciÃ³n.

---

# 11) Cheat Sheet

```bash
dbt deps
dbt compile
dbt build
dbt snapshot
dbt test
dbt docs generate
```

---

# Estado Final del Proyecto

âœ” Arquitectura por capas  
âœ” Dimensiones SCD Type 2  
âœ” Facts incrementales con MERGE  
âœ” Surrogate Keys  
âœ” Testing robusto  
âœ” DocumentaciÃ³n automÃ¡tica  
âœ” Preparado para CI/CD  
âœ” Estructura enterprise real  

---